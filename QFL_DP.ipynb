{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb67703-b72b-474e-8e62-e710cfdfa9dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from pyvacy import optim, analysis, sampling\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from torchvision import models\n",
    "from metaquantum.CircuitComponents import VariationalQuantumClassifierInterBlock_M_IN_N_OUT\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch import tensor\n",
    "import csv\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "import pennylane as qml\n",
    "import torch.multiprocessing as mp\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50a0dc36",
   "metadata": {},
   "source": [
    "Paramaters for QFL-DP Model and Initializaition of VQC from metaquantum inteface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e448ff-b808-4109-8b13-cbf1100cbc28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VariationalQuantumClassifierInterBlock_M_IN_N_OUT:\n",
    "\tdef __init__(\n",
    "\t\t\tself,\n",
    "\t\t\tnum_of_input= 10,\n",
    "\t\t\tnum_of_output= 4,\n",
    "\t\t\tnum_of_wires = 10,\n",
    "\t\t\tnum_of_layers = 2,\n",
    "\t\t\tvar_Q_circuit = None,\n",
    "\t\t\tvar_Q_bias = None,\n",
    "\t\t\tqdevice = \"default.qubit\",\n",
    "\t\t\thadamard_gate = False,\n",
    "\t\t\tmore_entangle = False,\n",
    "\t\t\tgpu = False):\n",
    "\n",
    "\t\tself.var_Q_circuit = var_Q_circuit\n",
    "\t\tself.var_Q_bias = var_Q_bias\n",
    "\t\tself.num_of_input = num_of_input\n",
    "\t\tself.num_of_output = num_of_output\n",
    "\t\tself.num_of_wires = num_of_wires\n",
    "\t\tself.num_of_layers = num_of_layers\n",
    "\n",
    "\t\tself.qdevice = qdevice\n",
    "\n",
    "\t\tself.hadamard_gate = hadamard_gate\n",
    "\t\tself.more_entangle = more_entangle\n",
    "\n",
    "\t\t\n",
    "\n",
    "\t\tif gpu == True and qdevice == \"qulacs.simulator\":\n",
    "\t\t\tprint(\"GOT QULACS AND GPU\")\n",
    "\t\t\tself.dev = qml.device(self.qdevice, wires = num_of_wires, gpu = True)\n",
    "\t\telse:\n",
    "\t\t\tself.dev = qml.device(self.qdevice, wires = num_of_wires)\n",
    "\n",
    "\n",
    "\tdef set_params(self, var_Q_circuit, var_Q_bias):\n",
    "\t\tself.var_Q_circuit = var_Q_circuit\n",
    "\t\tself.var_Q_bias = var_Q_bias\n",
    "\n",
    "\tdef init_params(self):\n",
    "\t\tself.var_Q_circuit = Variable(torch.tensor(0.01 * np.random.randn(self.num_of_layers, self.num_of_wires, 3), device=device).type(dtype), requires_grad=True)\n",
    "\t\treturn self.var_Q_circuit\n",
    "\n",
    "\tdef _statepreparation(self, angles):\n",
    "\n",
    "\t\t\"\"\"Quantum circuit to encode a the input vector into variational params\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\ta: feature vector of rad and rad_square => np.array([rad_X_0, rad_X_1, rad_square_X_0, rad_square_X_1])\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tif self.hadamard_gate == True:\n",
    "\t\t\tfor i in range(self.num_of_input):\n",
    "\t\t\t\tqml.Hadamard(wires=i)\n",
    "\n",
    "\t\tfor i in range(self.num_of_input):\n",
    "\t\t\tqml.RY(angles[i,0], wires=i)\n",
    "\t\t\tqml.RZ(angles[i,1], wires=i)\n",
    "\n",
    "\tdef _layer(self, W):\n",
    "\t\t\"\"\" Single layer of the variational classifier.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tW (array[float]): 2-d array of variables for one layer\n",
    "\n",
    "\t\t\"\"\"\n",
    "\n",
    "\n",
    "\t\t# Entanglement Layer\n",
    "\n",
    "\t\tfor i in range(self.num_of_wires):\n",
    "\t\t\tqml.CNOT(wires=[i, (i + 1) % self.num_of_wires])\n",
    "\n",
    "\t\tif self.more_entangle == True:\n",
    "\t\t\tfor j in range(self.num_of_wires):\n",
    "\t\t\t\tqml.CNOT(wires=[j, (j + 2) % self.num_of_wires])\n",
    "\n",
    "\t\t# Rotation Layer\n",
    "\t\tfor j in range(self.num_of_wires):\n",
    "\t\t\tqml.Rot(W[j, 0], W[j, 1], W[j, 2], wires=j)\n",
    "\n",
    "\tdef circuit(self, angles):\n",
    "\n",
    "\t\t@qml.qnode(self.dev, interface='torch')\n",
    "\t\tdef _circuit(var_Q_circuit, angles):\n",
    "\t\t\t\"\"\"The circuit of the variational classifier.\"\"\"\n",
    "\t\t\n",
    "\t\t\tself._statepreparation(angles)\n",
    "\n",
    "\t\t\tweights = var_Q_circuit\n",
    "\t\t\t\n",
    "\t\t\tfor W in weights:\n",
    "\t\t\t\tself._layer(W)\n",
    "\n",
    "\t\t\t\n",
    "\t\t\treturn [qml.expval(qml.PauliZ(k)) for k in range(self.num_of_output)]\n",
    "\n",
    "\t\treturn _circuit(self.var_Q_circuit, angles)\n",
    "\n",
    "\tdef _forward(self, angles):\n",
    "\t\t\"\"\"The variational classifier.\"\"\"\n",
    "\t\t\n",
    "\t\tbias = self.var_Q_bias \n",
    "\n",
    "\t\t\n",
    "\t\traw_output = self.circuit(angles)\n",
    "\n",
    "\t\t\n",
    "\t\t\n",
    "\t\treturn raw_output\n",
    "\n",
    "\tdef forward(self, angles):\n",
    "\t\n",
    "\t\tfw = self._forward(angles)\n",
    "\t\treturn fw\n",
    "\n",
    "dtype = torch.cuda.DoubleTensor if torch.cuda.is_available() else torch.DoubleTensor\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "qdevice = \"default.qubit\"\n",
    "\n",
    "params = {'isQuantum': True, 'usePyvacy': True, 'useGpu': True, \n",
    "         'epochs': 1, 'delta': 1e-05,'l2_clip': 0.1, \n",
    "          'l2_penalty': 0.001,'lr': 0.02, 'micro_bs': 16, 'mini_bs': 128, 'noise':1.5, 'noise_min' : 1.5,\n",
    "          'noise_max':2.0, 'noise_incr' : 0.5, 'rounds': 1, 'selected': 5,'clients':100, 'runs':1, \n",
    "         'eps_vs_acc': True, 'optimizer': 'SGD'}\n",
    "\n",
    "\n",
    "vqc = VariationalQuantumClassifierInterBlock_M_IN_N_OUT(\n",
    "    num_of_input=4,\n",
    "    num_of_output=2,\n",
    "    num_of_wires=4,\n",
    "    num_of_layers=2,\n",
    "    qdevice=qdevice,\n",
    "    hadamard_gate=False,\n",
    "    more_entangle=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c01bac7",
   "metadata": {},
   "source": [
    "Torch wrapper class for VQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f5f3a8-5458-4122-9448-1d5148cf6147",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    \n",
    "class VQCTorch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "        self.q_params = nn.Parameter(0.01 * torch.randn(2, 4, 3))\n",
    "\n",
    "    def get_angles_atan(self, in_x):\n",
    "        return torch.stack([torch.stack([torch.atan(item), torch.atan(item ** 2)]) for item in in_x])\n",
    "\n",
    "    def forward(self, batch_item):\n",
    "        # print('line 99')\n",
    "        vqc.var_Q_circuit = self.q_params\n",
    "        # print(self.vqc, 'at line 100')\n",
    "        score_batch = []\n",
    "\n",
    "        for single_item in batch_item:\n",
    "            res_temp = self.get_angles_atan(single_item)\n",
    "            # print(res_temp)\n",
    "\n",
    "            # print(self.vqc, 'at line 107')\n",
    "            q_out_elem = vqc.forward(res_temp)\n",
    "            # print(q_out_elem)\n",
    "\n",
    "            clamp = 1e-9\n",
    "            \n",
    "            # pdb.set_trace()\n",
    "            normalized_output = torch.clamp(torch.stack(q_out_elem), min=clamp)\n",
    "            score_batch.append(normalized_output)\n",
    "\n",
    "        scores = torch.stack(score_batch).view(len(batch_item), 2)\n",
    "\n",
    "        return scores\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90c9fb28",
   "metadata": {},
   "source": [
    "Data loading for Cats vs Dogs datasest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b209f5-928c-42fb-ab7b-81352d7a399b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(5),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop(\n",
    "            224, scale=(0.96, 1.0), ratio=(0.95, 1.05)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize([224, 224]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "####\n",
    "print(device)\n",
    "\n",
    "data_dir = '/global/u2/r/rr637/QPPAI-FL/data/'\n",
    "\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(\n",
    "    data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "\n",
    "\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "print(class_names)\n",
    "print(f\"Train image size: {dataset_sizes['train']}\")\n",
    "print(f'Validation image size: {dataset_sizes[\"val\"]}')\n",
    "####\n",
    "\n",
    "# Dividing the training data into num_clients, with each client having equal number of images\n",
    "\n",
    "traindata = image_datasets['train']\n",
    "client_train_size = int(dataset_sizes['train'] / params['clients'])\n",
    "print(f\"client_train_size: {client_train_size}\")\n",
    "\n",
    "if params['usePyvacy']:\n",
    "    epsilon = analysis.epsilon(dataset_sizes['train'],params['mini_bs'],\n",
    "                                    params['noise'], params['rounds'],params['delta'])\n",
    "    params['epsilon'] = epsilon\n",
    "    \n",
    "    print(epsilon)\n",
    "\n",
    "traindata_split = torch.utils.data.random_split(traindata,\n",
    "                                                [client_train_size for _ in range(params['clients'])])\n",
    "\n",
    "# Creating a pytorch loader for a Deep Learning model\n",
    "train_loader = [torch.utils.data.DataLoader(\n",
    "    x, batch_size=params['mini_bs'], shuffle=True) for x in traindata_split]\n",
    "\n",
    "\n",
    "\n",
    "# Loading the test iamges and thus converting them into a test_loader\n",
    "testdata = image_datasets['val']\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    testdata, batch_size=params['mini_bs'], shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hybrid-quantum classical model and benchmark fully classical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d567062-3674-4a98-8b64-e07e7f555aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QuantumTransfer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = torchvision.models.vgg16(weights='VGG16_Weights.DEFAULT')\n",
    "        for param in self.net.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.net.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=25088, out_features=4096, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Linear(in_features=4096, out_features=4096, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Linear(in_features=4096, out_features=4, bias=True),\n",
    "            VQCTorch())\n",
    "\n",
    "    def forward(self, in_x):\n",
    "        return self.net(in_x)\n",
    "\n",
    "class ClassicalTransfer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = torchvision.models.vgg16(weights='VGG16_Weights.DEFAULT')\n",
    "        for param in self.net.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.net.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=25088, out_features=4096, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Linear(in_features=4096, out_features=4096, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Linear(in_features=4096, out_features=4, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=4, out_features=2, bias=True),\n",
    "        )\n",
    "            \n",
    "\n",
    "    def forward(self, in_x):\n",
    "        return self.net(in_x)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "528097e6",
   "metadata": {},
   "source": [
    "Differentially private local updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46939e-6dbf-4303-9b3a-11471ea79f6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def client_update(client_model, optimizer, train_loader, epoch, index, r):\n",
    "    \"\"\"\n",
    "    This function updates/trains client model on client data with differential privacy\n",
    "    \"\"\"\n",
    "\n",
    "    client_model.train()\n",
    "    client_model = client_model.to(device)\n",
    "    loss_list = []\n",
    "    acc_list =[]\n",
    "    if params['usePyvacy']:\n",
    "        \n",
    "        for e in range(params['epochs']):\n",
    "            print(\"EPOCH: \", e)\n",
    "            acc_mini = []\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                # print(\"BATCH IDX: \", batch_idx)\n",
    "                data, target = data.to(device), target.to(device)\n",
    "               \n",
    "                optimizer.zero_grad()\n",
    "                acc_micro = []\n",
    "                for microbatch_idx, (microbatch_data, microbatch_target) in enumerate(\n",
    "                        zip(data.split(optimizer.microbatch_size), target.split(optimizer.microbatch_size))):\n",
    "#                    \n",
    "                    microbatch_data, microbatch_target = microbatch_data.to(\n",
    "                        device), microbatch_target.to(device)\n",
    "#                     print(microbatch_data.dtype, microbatch_target.dtype)\n",
    "#                     if (params['useGpu'] and not microbatch_data.is_cuda):\n",
    "#                         print(\"moving microbatch to cuda...\")\n",
    "#                         microbatch_data = microbatch_data.cuda()\n",
    "#                         microbatch_target = microbatch_target.cuda()\n",
    "                    optimizer.zero_microbatch_grad() \n",
    "                    output = client_model(microbatch_data)\n",
    "                    criterion = nn.CrossEntropyLoss()\n",
    "#        \n",
    "                    loss = criterion(output, microbatch_target)\n",
    "                    \n",
    "                    # print(\"WithDP-Loss: \", loss.item())\n",
    "                   \n",
    "                    loss.backward()\n",
    "#                     client_model.update_l2_norm_list(optimizer, isDP=False)\n",
    "                    optimizer.microbatch_step()\n",
    "                    preds = torch.argmax(output, dim=1).to(device)\n",
    "                    # print(f\"Shape of target: {microbatch_target.shape} and preds: {preds.shape}\", microbatch_target, preds)\n",
    "                    \n",
    "                    accuracy = accuracy_score(microbatch_target.cpu().numpy(), preds.cpu().numpy())\n",
    "                    acc_micro.append(accuracy)\n",
    "                    # print(\"Accuracy = {}\".format(accuracy))\n",
    "                    \n",
    "                    \n",
    "            \n",
    "            \n",
    "                    \n",
    "                    \n",
    "                micro_avg = sum(acc_micro)/len(acc_micro)\n",
    "                acc_mini.append(micro_avg)\n",
    "                optimizer.step()\n",
    "#                 client_model.update_l2_norm_list(optimizer, isDP=True)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            \n",
    "                # correct += output.eq(target.view_as(output)).sum().item()\n",
    "            # acc = correct / len(client_test_loader.dataset)\n",
    "            mini_avg = sum(acc_mini)/len(acc_mini)\n",
    "            acc_list.append(mini_avg)\n",
    "            loss_list.append(loss.item())\n",
    "            \n",
    "        \n",
    "            \n",
    "            print(f\"Loss per epoch: {loss.item()}\")\n",
    "            print(f\"Epoch {e} Accuracy: {mini_avg}\")\n",
    "                \n",
    "    else:\n",
    "            \n",
    "        for e in range(epoch):\n",
    "            acc_mini = []\n",
    "            print(\"Client: \",index+1)\n",
    "            acc_mini=[]\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                # print(\"BATCH IDX: \", batch_idx)\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = client_model(data)\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "                # loss = F.nll_loss(output, target)\n",
    "                loss = criterion(output, target)\n",
    "                # print(\"withoutDP-Loss: \", loss.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                preds = torch.argmax(output, dim=1).to(device)\n",
    "                accuracy = accuracy_score(target.cpu().numpy(), preds.cpu().numpy())\n",
    "                print(f\"accuracy_per_mini: {accuracy}\")\n",
    "                acc_mini.append(accuracy)\n",
    "        \n",
    "            \n",
    "            print(f\"Loss per epoch: {loss.item()}\")\n",
    "            print(f\"acc_mini_list: {acc_mini}\")\n",
    "            mini_avg = sum(acc_mini)/len(acc_mini)\n",
    "            print(f\"Epoch {e} Accuracy: {mini_avg}\")\n",
    "            acc_list.append(mini_avg)\n",
    "            loss_list.append(loss.item())\n",
    "    avg_acc = sum(acc_list)/params['epochs']\n",
    "    return loss.item(), avg_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local model aggregation and sending of global model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524127a9-5a79-4f05-998f-fad5af9f2d93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def server_aggregate(global_model, client_models):\n",
    "    \"\"\"\n",
    "    This function has aggregation method 'mean'\n",
    "    \"\"\"\n",
    "    ### This will take simple mean of the weights of models ###\n",
    "   \n",
    "    global_dict = global_model.state_dict()\n",
    "    for k in global_dict.keys():\n",
    "            global_dict[k] = torch.stack([client_models[i].state_dict()[k].float() for i in range(len(client_models))],\n",
    "                                        0).mean(0)\n",
    "    global_model.load_state_dict(global_dict)\n",
    "    for model in client_models:\n",
    "            \n",
    "        model.load_state_dict(global_model.state_dict())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c95b03c",
   "metadata": {},
   "source": [
    "Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98027b5-60a9-4d75-ad7e-0a05ee73f3d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def test(global_model, test_loader):\n",
    "    \"\"\"This function test the global model on test data and returns test loss and test accuracy \"\"\"\n",
    "    global_model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = global_model(data.to(torch.float))\n",
    "            criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "            # test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            test_loss += criterion(output, target).item()\n",
    "            # test_loss.append(criterion(output, target).item())\n",
    "            # get the index of the max log-probability\n",
    "           \n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    acc = correct / len(test_loader.dataset)\n",
    "\n",
    "    return test_loss, acc\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d41c640d",
   "metadata": {},
   "source": [
    "Saving and plotting of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba410b2-3d15-4d92-a017-c7b58829ced7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def saving_and_plotting(time, model_lists, ltr_lists, lt_lists, at_lists, atr_lists,params):\n",
    "    # DO not save the pretrained VGG part\n",
    "    runs = len(model_lists)\n",
    "    isQuantum = params['isQuantum']\n",
    "    exp_name = f'{time}_Quantum:_{isQuantum}'\n",
    "    directory = f\"Exp:_{exp_name}\"\n",
    "    parent_dir = '/global/u2/r/rr637/QPPAI-FL/Results'\n",
    "    path = os.path.join(parent_dir, directory)\n",
    "    os.mkdir(path)\n",
    "    path_plots = path + '/Plots'\n",
    "    os.mkdir(path_plots)\n",
    "    path_models = path + '/Models'\n",
    "    os.mkdir(path_models)\n",
    "    for i in range(runs):\n",
    "        plot_acc_loss(atr_lists[i],ltr_lists[i],at_lists[i],lt_lists[i], params['isQuantum'], \n",
    "                      exp_name,f'Accuracy/Loss per Round - Iteration {i+1}',i+1)\n",
    "        torch.save(model_lists[i].net.classifier.state_dict(), f\"{path_models}/model_iteration={i}\")\n",
    "    lt_ave = list(np.average(np.array(lt_lists),axis = 0))\n",
    "    at_ave = list(np.average(np.array(at_lists),axis = 0))\n",
    "    ltr_ave = list(np.average(np.array(ltr_lists),axis = 0))\n",
    "    atr_ave = list(np.average(np.array(atr_lists),axis = 0))\n",
    "    if runs>1:\n",
    "        plot_acc_loss(atr_ave,ltr_ave,at_ave,lt_ave,params['isQuantum'],exp_name,\n",
    "                      f'Averaged Accuracy/Loss per Round After {runs} Iterations',0)\n",
    "    \n",
    "  \n",
    "        \n",
    "\n",
    "# Save lists as CSV files\n",
    "    with open(path + \"/avg_loss_test.csv\", \"w\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(lt_ave)\n",
    "\n",
    "    with open(path + \"/avg_loss_train.csv\", \"w\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(ltr_ave)\n",
    "\n",
    "    with open(path + \"/avg_acc_test.csv\", \"w\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(at_ave)\n",
    "\n",
    "    with open(path + \"/avg_acc_train.csv\", \"w\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(atr_ave)\n",
    "\n",
    "    # Save the dictionary as a CSV file\n",
    "    with open(path + \"/params.csv\", \"w\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for key, value in params.items():\n",
    "            writer.writerow([key, value])\n",
    "\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def plot_epsilon_v_acc(epsilon_list, acc_list,exp_index,title):\n",
    "    plt.scatter(epsilon_list,acc_list),\n",
    "    plt.xlabel('epsilon'),\n",
    "    plt.ylabel('final_test_accuracy'),\n",
    "    plt.title(title),\n",
    "    plt.savefig(f\"Results/Epsilon_vs_Acc/{exp_index}_avg_epsilon_vs_accuracy.png\"),\n",
    "    plt.show()\n",
    "       \n",
    "def plot_acc_loss(tr_a,tr_l,t_a,t_l,is_Quantum, exp_name,title,i):\n",
    "    plt.plot(tr_a,label = \"train accuracy\"),\n",
    "    plt.plot(tr_l,label = \"train loss\"),\n",
    "    plt.plot(t_a,label = \"test accuracy\"),\n",
    "    plt.plot(t_l,label = \"test loss\"),\n",
    "    plt.xlabel('Round'),\n",
    "    plt.ylabel('Accuracy/Loss'),\n",
    "    plt.title(title),\n",
    "    plt.legend(),\n",
    "    plt.savefig(f\"Results/Exp:_{exp_name}/Plots/Acc_loss_plot_{i}.png\"),\n",
    "    plt.show()             \n",
    "             \n",
    "             \n",
    "    \n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67bcd103",
   "metadata": {},
   "source": [
    "Main function for QFL-DP training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ab82a-af3e-4a68-80fc-3619b16a3f7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def main(params):\n",
    "\n",
    "    if params['isQuantum']:\n",
    "        global_model = QuantumTransfer().to(device)\n",
    "        client_models = [QuantumTransfer().to(device)\n",
    "                     for _ in range(params['selected'])]\n",
    "    else:\n",
    "        global_model = ClassicalTransfer().to(device)\n",
    "        client_models = [ClassicalTransfer().to(device)\n",
    "                     for _ in range(params['selected'])]\n",
    "        \n",
    "    for model in client_models:\n",
    "        # initial synchronizing with global model\n",
    "        model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "    if params['usePyvacy'] == True:\n",
    "\n",
    "        opt = [optim.DPSGD(\n",
    "            l2_norm_clip= params['l2_clip'],\n",
    "            noise_multiplier= params['noise'],\n",
    "            minibatch_size= params['mini_bs'],\n",
    "            microbatch_size= params['micro_bs'],\n",
    "            params=model.net.classifier.parameters(),\n",
    "            lr = params['lr'],\n",
    "            weight_decay= params['l2_penalty'])\n",
    "         for model in client_models]\n",
    "\n",
    "\n",
    "    else:\n",
    "        opt = [optim.SGD(model.net.classifier.parameters(), lr=params['lr']) for model in client_models]\n",
    "\n",
    "    losses_train = []\n",
    "    losses_test = []\n",
    "    acc_train = []\n",
    "    acc_test = []\n",
    "    if params['usePyvacy']:\n",
    "        epsilon = params['epsilon']\n",
    "        delta = params['delta']\n",
    "        print(f'Achieves ({epsilon}, {delta})-DP')\n",
    "\n",
    "    for r in range(params['rounds']):\n",
    "        # select random clients\n",
    "        num_selected = params['selected']\n",
    "        client_idx = np.random.permutation(params['clients'])[:num_selected]\n",
    "        # client update\n",
    "        loss = 0\n",
    "        acc = 0\n",
    "        for i in range(params['selected']):\n",
    "            loss_temp, acc_temp = client_update(client_models[i], opt[i],\n",
    "                                  train_loader[client_idx[i]], epoch=params['epochs']\n",
    "                                                , index = i, r=r)\n",
    "            loss += loss_temp\n",
    "            acc += acc_temp\n",
    "        losses_train.append(loss / params['selected'])\n",
    "        acc_train.append(acc/params['selected'])\n",
    "       \n",
    "        trained_client_models = client_models\n",
    "        trained_global_model = global_model\n",
    "        server_aggregate(trained_global_model, trained_client_models)\n",
    "        test_loss, test_acc = test(global_model, test_loader)\n",
    "        losses_test.append(test_loss)\n",
    "        acc_test.append(test_acc)\n",
    "        print(f\"loss_test_list_per_round: {losses_test}\")\n",
    "        print(f\"acc_test_list_per_round: {acc_test}\")\n",
    "    print(f\"FINAL TEST ACC : {acc_test[-1]} After {params['rounds']} Rounds\")\n",
    "\n",
    "\n",
    "    final_dict = {}\n",
    "    final_dict['lt'] = losses_test\n",
    "    final_dict['at'] = acc_test\n",
    "    final_dict['ltr'] = losses_train\n",
    "    final_dict['atr'] = acc_train\n",
    "    final_dict['model'] = global_model\n",
    "\n",
    "    return final_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04ef330f",
   "metadata": {},
   "source": [
    "Script for running experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3d4b7b-002f-45ea-a5ff-0d1b1a580461",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_index = datetime.now().strftime(\"%m_%d_%H_%M_%S\")\n",
    "if not params['eps_vs_acc']:\n",
    "    \n",
    "    lt_lists=[]\n",
    "    at_lists=[]\n",
    "    ltr_lists=[]\n",
    "    atr_lists=[]\n",
    "    model_lists = []\n",
    "    for i in range(params['runs']):\n",
    "        results = main(params)\n",
    "        lt_lists.append(results['lt'])\n",
    "        at_lists.append(results['at'])\n",
    "        ltr_lists.append(results['ltr'])\n",
    "        atr_lists.append(results['atr'])\n",
    "        model_lists.append(results['model'])\n",
    "    saving_and_plotting(exp_index, model_lists, ltr_lists, lt_lists, at_lists, atr_lists, params)\n",
    "        \n",
    "\n",
    "else:\n",
    "    fin_acc_lists =[]\n",
    "    epsilon_lists= []\n",
    "    noise_min = params['noise_min']\n",
    "    noise_max = params['noise_max']\n",
    "    noise_incr = params['noise_incr']\n",
    "    \n",
    "    for i in range(params['runs']):\n",
    "        epsilon_list = []\n",
    "        final_accuracy_list =[]\n",
    "       \n",
    "        for noise in np.arange(noise_min, noise_max, noise_incr): #exclusive\n",
    "            params['noise'] = noise\n",
    "            dict = main(params)\n",
    "            final_accuracy_list.append(dict['at'][-1])\n",
    "            epsilon = analysis.epsilon(dataset_sizes['train'],params['mini_bs'],\n",
    "                                noise, params['rounds'],params['delta'])\n",
    "            epsilon_list.append(epsilon)\n",
    "        fin_acc_lists.append(final_accuracy_list)\n",
    "        epsilon_lists.append(epsilon_list)\n",
    "    fin_acc_avg = list(np.average(np.array(fin_acc_lists),axis = 0))\n",
    "    eps_ave = list(np.average(np.array(epsilon_lists),axis = 0))  \n",
    "    print(fin_acc_lists)\n",
    "    print(fin_acc_avg)\n",
    "    print(epsilon_lists)\n",
    "    print(eps_ave)\n",
    "    runs = params['runs']\n",
    "    plot_epsilon_v_acc(eps_ave, fin_acc_avg,exp_index,f'Averaged Epsilon vs Acc After {runs} Iterations')                               \n",
    "    eps_v_acc_path = '/global/u2/r/rr637/QPPAI-FL/Results/Epsilon_vs_Acc'\n",
    "    with open(eps_v_acc_path + \"/params\" + \".txt\", \"wb\") as fp:\n",
    "        pickle.dump(params, fp)\n",
    "        \n",
    "        \n",
    "                \n",
    "        \n",
    "        \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a332f068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
